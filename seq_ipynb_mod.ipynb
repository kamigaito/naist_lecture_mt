{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snGTTqJflyR0"
      },
      "source": [
        "### Check the assigned GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOSGwKwlxyW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsJRzbOgZ9UN"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yERmi_cfOSzE"
      },
      "source": [
        "!pip install ConfigArgParse\n",
        "!pip install torchtext==0.4.0\n",
        "!apt-get install file autoconf libtool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSHf2qY9bQ09"
      },
      "source": [
        "### Set environmental variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-fddmQwbPr2"
      },
      "source": [
        "import os\n",
        "pwd = os.getcwd()\n",
        "### This directory contains libraries and exexution files.\n",
        "APPS_DIR = pwd + '/apps'\n",
        "%env APPS_DIR=$APPS_DIR\n",
        "### This directory contains training, development and test set\n",
        "DATA_DIR = pwd + '/dataset'\n",
        "%env DATA_DIR=$DATA_DIR\n",
        "### This directory contains output directions.\n",
        "WORK_DIR = pwd + '/work'\n",
        "%env WORK_DIR=$WORK_DIR\n",
        "\n",
        "#japanese -> english\n",
        "%env LANG_PAIR=ja-en\n",
        "#english -> japanese\n",
        "#%env LANG_PAIR=en-ja\n",
        "\n",
        "%env DATASET=$DATA_DIR/kftt-data-1.0/data/tok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8GWDwl-qtS4"
      },
      "source": [
        "### Install applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-jvJ2a5aWDu"
      },
      "source": [
        "%%shell\n",
        "\n",
        "echo \"download and make apps...\"\n",
        "\n",
        "mkdir -p ${APPS_DIR}\n",
        "cd ${APPS_DIR}\n",
        "\n",
        "git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "git clone https://github.com/OpenNMT/OpenNMT-py.git -b 1.2.0\n",
        "git clone https://github.com/neubig/kytea.git\n",
        "cd kytea\n",
        "autoreconf -i\n",
        "./configure --prefix=${APPS_DIR}/kytea\n",
        "make\n",
        "make install\n",
        "\n",
        "echo \"finish apps preparation.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2YWdq3qykg"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhZjOtao95lS"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p $WORK_DIR/input\n",
        "mkdir -p ${DATA_DIR}\n",
        "\n",
        "# Download data set\n",
        "wget -O ${DATA_DIR}/kftt-data-1.0.tar.gz http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
        "cd ${DATA_DIR}\n",
        "# Uncompress\n",
        "tar xvzf kftt-data-1.0.tar.gz\n",
        "\n",
        "paste \\\n",
        "  ${DATASET}/kyoto-train.cln.en \\\n",
        "  ${DATASET}/kyoto-train.cln.ja |\n",
        "  shuf > ${DATASET}/train.shuf\n",
        "\n",
        "#1p\n",
        "#head -n $((`cat ${DATASET}/train.shuf | wc -l`/100)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
        "#head -n $((`cat ${DATASET}/train.shuf | wc -l`/100)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
        "#20p\n",
        "head -n $((`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
        "head -n $((`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
        "#60p\n",
        "#head -n $((3*`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
        "#head -n $((3*`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
        "#100p\n",
        "#cut -f 1 ${DATASET}/train.shuf > ${DATASET}/train.en\n",
        "#cut -f 2 ${DATASET}/train.shuf > ${DATASET}/train.ja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_pyMUHrW2tu"
      },
      "source": [
        "### See sample sentences in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itZl5fdyVj1-"
      },
      "source": [
        "%%shell\n",
        "\n",
        "train_name=train\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "sed -n 1P ${DATASET}/${train_name}.${src}\n",
        "sed -n 1P ${DATASET}/${train_name}.${trg}\n",
        "sed -n 2P ${DATASET}/${train_name}.${src}\n",
        "sed -n 2P ${DATASET}/${train_name}.${trg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDICQ3RJW_S2"
      },
      "source": [
        "### Preprocessing of OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8KpDJetNPPl"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "suffix=\"en-ja ja-en\"\n",
        "train_name=train\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "    python preprocess.py \\\n",
        "    -train_src ${DATASET}/${train_name}.${src} \\\n",
        "    -train_tgt ${DATASET}/${train_name}.${trg} \\\n",
        "    -valid_src ${DATASET}/kyoto-dev.${src} \\\n",
        "    -valid_tgt ${DATASET}/kyoto-dev.${trg} \\\n",
        "    -save_data ${DATA_DIR}/dicts-${train_name}-${LANG_PAIR} \\\n",
        "    -src_words_min_frequency 5 \\\n",
        "    -tgt_words_min_frequency 5 \\\n",
        "    -src_seq_length 40 \\\n",
        "    -tgt_seq_length 40\n",
        "#train_src, train_tgt, valid_src, valid_tgt: text\n",
        "#save_data: bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v_K4UgrXNeP"
      },
      "source": [
        "### Check the preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q_WvnoV9BPa"
      },
      "source": [
        "%cd {APPS_DIR}/OpenNMT-py\n",
        "import torch\n",
        "\n",
        "# Training data\n",
        "data = torch.load('/content/dataset/dicts-train-ja-en.train.0.pt')\n",
        "\n",
        "# Vocabulary\n",
        "vocab = torch.load('/content/dataset/dicts-train-ja-en.vocab.pt')\n",
        "\n",
        "print(\"Sentence number of training data: \", len(data))\n",
        "print(\"\")\n",
        "print(\"data[0]\")\n",
        "print(\"index: \", data[0].indices)\n",
        "print(\"Source: \", data[0].src)\n",
        "print(\"Translated: \", data[0].tgt)\n",
        "print(\"\")\n",
        "print(\"data[1]\")\n",
        "print(\"index: \", data[1].indices)\n",
        "print(\"Source: \", data[1].src)\n",
        "print(\"Translated: \", data[1].tgt)\n",
        "print(\"\")\n",
        "print(\"The above input is transformed into vectors of indices as follows:\")\n",
        "print(vocab['src'].fields[0][1].process((data[0].src[0],data[1].src[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMkzauOrq3MP"
      },
      "source": [
        "### Run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRlSf2dGKqOV"
      },
      "source": [
        "%%shell\n",
        "\n",
        "start_time=`date +%s`\n",
        "\n",
        "TIME_PATH=$(pwd)/train.time.log\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_NAME=train\n",
        "MODEL_DIR=${WORK_DIR}/models\n",
        "\n",
        "NUM_EPOCH=10\n",
        "\n",
        "mkdir -p ${MODEL_DIR}\n",
        "\n",
        "NUM_DATA=$(cat ${DATASET}/${TRAIN_NAME}.en | wc -l)\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "\n",
        "batch_size=256\n",
        "num_steps=$((${NUM_DATA}*${NUM_EPOCH}/${batch_size}))\n",
        "# seq2seq with attention (LSTM-based)\n",
        "python train.py \\\n",
        "    -data ${DATA_DIR}/dicts-${TRAIN_NAME}-${LANG_PAIR} \\\n",
        "    -save_model ${MODEL_DIR}/${LANG_PAIR} \\\n",
        "    -layers 2 \\\n",
        "    -rnn_size 500 \\\n",
        "    -word_vec_size 300 \\\n",
        "    -optim adam \\\n",
        "    -learning_rate 0.001 \\\n",
        "    -dropout 0.3 \\\n",
        "    -batch_size ${batch_size} \\\n",
        "    -report_every 1 \\\n",
        "    -save_checkpoint_steps ${num_steps} \\\n",
        "    -train_steps ${num_steps} \\\n",
        "    -gpu_rank 0 |& awk '(NR%30==0 || NR < 70){print}' # Show partial logs\n",
        "cp ${MODEL_DIR}/${LANG_PAIR}_step_${num_steps}.pt ${MODEL_DIR}/${LANG_PAIR}_final.pt\n",
        "\n",
        "\n",
        "end_time=`date +%s`\n",
        "time=$((end_time - start_time))\n",
        "echo \"${time} (sec)\" >& $TIME_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "capkxxMCmXVF"
      },
      "source": [
        "### Prediction (Translation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qAuwS9VC1jU"
      },
      "source": [
        "%%shell\n",
        "\n",
        "\n",
        "MODELS=${WORK_DIR}/models\n",
        "OUT_DIR=${WORK_DIR}/outputs\n",
        "USRDIR=${WORK_DIR}/input\n",
        "SCORE_PATH=$(pwd)/score.txt\n",
        "\n",
        "mkdir -p ${OUT_DIR}\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "# Translating sentences by greedy decoding\n",
        "python translate.py \\\n",
        "    -model ${MODELS}/${LANG_PAIR}_final.pt \\\n",
        "    -src ${DATASET}/kyoto-test.${src} \\\n",
        "    -output ${OUT_DIR}/test.${trg} \\\n",
        "    -gpu 0 \\\n",
        "    -beam_size 1 \\\n",
        "    -batch_size 512 \\\n",
        "    -verbose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation (Copy the output and report it)"
      ],
      "metadata": {
        "id": "mSzv8BcTE7u1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmtWMzaDm0qg"
      },
      "source": [
        "%%shell\n",
        "\n",
        "\n",
        "MODELS=${WORK_DIR}/models\n",
        "OUT_DIR=${WORK_DIR}/outputs\n",
        "USRDIR=${WORK_DIR}/input\n",
        "SCORE_PATH=$(pwd)/score.txt\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "for i in 1 2 3 4 5; do\n",
        "  echo [Generated translation $i]\n",
        "  sed -n ${i}P ${OUT_DIR}/test.${trg}\n",
        "  echo [Reference translation $i]\n",
        "  sed -n ${i}P ${DATASET}/kyoto-test.${trg}\n",
        "done\n",
        "\n",
        "# Calculate BLEU score\n",
        "perl ${APPS_DIR}/mosesdecoder/scripts/generic/multi-bleu.perl \\\n",
        "    ${DATASET}/kyoto-test.${trg} \\\n",
        "    < ${OUT_DIR}/test.${trg} \\\n",
        "    1> ${OUT_DIR}/result_${LANG_PAIR}.bleu \\\n",
        "    2> /dev/null\n",
        "\n",
        "cat ${OUT_DIR}/result_${LANG_PAIR}.bleu | sed -r 's/(BLEU = [0-9]*\\.[0-9]*), .*/\\1/g' | tee ${SCORE_PATH} > /dev/null\n",
        "echo \"------------------------------\"\n",
        "nvidia-smi\n",
        "cat train.time.log\n",
        "cat score.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzvx40A5Eaer"
      },
      "source": [
        "### Advanced: translating your text (no obligation to run)\n",
        "#### You can edit the below part `echo 何らかの文章をここに書く。 > ${USRDIR}/user.${src}` to translate your own Japanese text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoExgq2kKqiB"
      },
      "source": [
        "%%shell\n",
        "MODELS=${WORK_DIR}/models\n",
        "OUT_DIR=${WORK_DIR}/outputs\n",
        "USRDIR=${WORK_DIR}/input\n",
        "SCORE_PATH=$(pwd)/score.txt\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "echo 何らかの文章をここに書く。 > ${USRDIR}/user.${src}\n",
        "\n",
        "if [ ! -e ${USRDIR}/user.${src} ]\n",
        "then\n",
        "    echo \"${USRDIR}/user.${src} does not exist.\"\n",
        "    exit\n",
        "fi\n",
        "\n",
        "# Tokenization\n",
        "if [ ${src} = \"ja\" -a ${src} != \"en\" ]\n",
        "then\n",
        "    # Tokenize Japanese sentences\n",
        "    bash ${APPS_DIR}/kytea/src/bin/kytea \\\n",
        "    < ${USRDIR}/user.${src} |\\\n",
        "    sed 's/\\/[^ ]\\+//g' \\\n",
        "    > ${USRDIR}/user.tok.${src}\n",
        "elif [ ${src} = \"en\" -a ${src} != \"ja\" ]\n",
        "then\n",
        "    # Tokenize English sentences\n",
        "    perl ${APPS_DIR}/mosesdecoder/scripts/tokenizer/tokenizer.perl \\\n",
        "    -l en \\\n",
        "    < ${USRDIR}/user.${src} \\\n",
        "    > ${USRDIR}/user.tok.${src}\n",
        "else\n",
        "echo \"Language: ${src} is undefined.\"\n",
        "    continue\n",
        "fi\n",
        "\n",
        "# Tokenized text\n",
        "cat ${USRDIR}/user.tok.${src}\n",
        "\n",
        "# Do translation\n",
        "python translate.py \\\n",
        "    -model ${MODELS}/${LANG_PAIR}_final.pt \\\n",
        "    -src ${USRDIR}/user.tok.${src} \\\n",
        "    -output ${OUT_DIR}/user.${trg} \\\n",
        "    -gpu 0 \\\n",
        "    -beam_size 1 \\\n",
        "    -batch_size 512 \\\n",
        "    -verbose"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}